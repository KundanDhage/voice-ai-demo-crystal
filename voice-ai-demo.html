<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI Demo - Crystal</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
        }

        .container {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            max-width: 600px;
            width: 90%;
            text-align: center;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        h1 {
            margin-bottom: 10px;
            font-size: 2.5em;
            background: linear-gradient(45deg, #fff, #f0f0f0);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .subtitle {
            margin-bottom: 30px;
            opacity: 0.8;
            font-size: 1.1em;
        }

        .voice-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(45deg, #ff6b6b, #ff8e8e);
            color: white;
            font-size: 3em;
            cursor: pointer;
            transition: all 0.3s ease;
            margin: 20px;
            box-shadow: 0 10px 30px rgba(255, 107, 107, 0.4);
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }

        .voice-button:hover {
            transform: scale(1.05);
            box-shadow: 0 15px 40px rgba(255, 107, 107, 0.6);
        }

        .voice-button.listening {
            background: linear-gradient(45deg, #4ecdc4, #44a08d);
            animation: pulse 1.5s infinite;
        }

        .voice-button.processing {
            background: linear-gradient(45deg, #ffa726, #ff9800);
            animation: spin 2s linear infinite;
        }

        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .status {
            margin: 20px 0;
            font-size: 1.2em;
            min-height: 30px;
            font-weight: 500;
        }

        .transcript-area {
            margin: 30px 0;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 20px;
            min-height: 150px;
            text-align: left;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .transcript-entry {
            margin: 10px 0;
            padding: 10px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 8px;
            border-left: 4px solid #4ecdc4;
        }

        .user-message {
            border-left-color: #ff6b6b;
        }

        .ai-message {
            border-left-color: #4ecdc4;
        }

        .config-section {
            margin: 20px 0;
            text-align: left;
        }

        .config-input {
            width: 100%;
            padding: 10px;
            margin: 5px 0;
            border: none;
            border-radius: 8px;
            background: rgba(255, 255, 255, 0.1);
            color: white;
            font-size: 14px;
        }

        .config-input::placeholder {
            color: rgba(255, 255, 255, 0.6);
        }

        .config-input:focus {
            outline: none;
            background: rgba(255, 255, 255, 0.2);
        }

        .clear-btn {
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.3);
            color: white;
            padding: 10px 20px;
            border-radius: 8px;
            cursor: pointer;
            margin: 10px 5px;
            transition: all 0.3s ease;
        }

        .clear-btn:hover {
            background: rgba(255, 255, 255, 0.2);
        }

        .error {
            color: #ff6b6b;
            background: rgba(255, 107, 107, 0.1);
            padding: 10px;
            border-radius: 8px;
            margin: 10px 0;
        }

        .success {
            color: #4ecdc4;
            background: rgba(78, 205, 196, 0.1);
            padding: 10px;
            border-radius: 8px;
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ Voice AI Demo</h1>
        <p class="subtitle">Crystal AI Generalist Challenge</p>
        
        <div class="config-section">
            <input type="password" class="config-input" id="openai-key" placeholder="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" />
            <input type="text" class="config-input" id="google-sheet-url" placeholder="Google Sheet URL (Optional)" />
        </div>

        <button class="voice-button" id="voiceBtn">üé§</button>
        
        <div class="status" id="status">Click the microphone to start</div>
        
        <div class="transcript-area" id="transcriptArea">
            <div style="text-align: center; opacity: 0.6;">Conversation will appear here...</div>
        </div>

        <button class="clear-btn" onclick="clearTranscript()">Clear Transcript</button>
        <button class="clear-btn" onclick="downloadTranscript()">Download Log</button>
    </div>

    <script>
        class VoiceAIDemo {
            constructor() {
                this.recognition = null;
                this.isListening = false;
                this.isProcessing = false;
                this.conversationLog = [];
                this.init();
            }

            init() {
                // Check for speech recognition support
                if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                    this.showError('Speech recognition not supported in this browser. Please use Chrome.');
                    return;
                }

                // Initialize speech recognition
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                this.recognition = new SpeechRecognition();
                this.recognition.continuous = false;
                this.recognition.interimResults = false;
                this.recognition.lang = 'en-US';

                this.setupEventListeners();
                this.updateStatus('Ready to listen! Click the microphone to start.');
            }

            setupEventListeners() {
                const voiceBtn = document.getElementById('voiceBtn');
                voiceBtn.addEventListener('click', () => this.toggleListening());

                this.recognition.onstart = () => {
                    this.isListening = true;
                    this.updateButton();
                    this.updateStatus('Listening... Speak now!');
                };

                this.recognition.onresult = (event) => {
                    const transcript = event.results[0][0].transcript;
                    this.processUserInput(transcript);
                };

                this.recognition.onerror = (event) => {
                    this.showError(`Speech recognition error: ${event.error}`);
                    this.resetState();
                };

                this.recognition.onend = () => {
                    this.isListening = false;
                    this.updateButton();
                };
            }

            toggleListening() {
                if (this.isProcessing) return;

                if (this.isListening) {
                    this.recognition.stop();
                } else {
                    this.startListening();
                }
            }

            startListening() {
                const apiKey = document.getElementById('openai-key').value.trim();
                if (!apiKey) {
                    this.showError('Please enter your OpenAI API key first!');
                    return;
                }

                try {
                    this.recognition.start();
                } catch (error) {
                    this.showError('Could not start speech recognition. Please try again.');
                }
            }

            async processUserInput(userText) {
                this.isProcessing = true;
                this.updateButton();
                this.updateStatus('Processing your request...');

                // Add user message to transcript
                this.addToTranscript('User', userText);

                try {
                    // Get AI response
                    const aiResponse = await this.getAIResponse(userText);
                    
                    // Add AI response to transcript
                    this.addToTranscript('AI', aiResponse);
                    
                    // Speak the response
                    await this.speakText(aiResponse);
                    
                    // Log to external service if configured
                    this.logToExternalService(userText, aiResponse);
                    
                } catch (error) {
                    this.showError('Error processing request: ' + error.message);
                }

                this.resetState();
            }

            async getAIResponse(userText) {
                const apiKey = document.getElementById('openai-key').value.trim();
                
                const response = await fetch('https://api.openai.com/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: JSON.stringify({
                        model: 'gpt-3.5-turbo',
                        messages: [
                            {
                                role: 'system',
                                content: 'You are a helpful AI assistant. Keep responses concise and conversational, typically 1-2 sentences.'
                            },
                            {
                                role: 'user',
                                content: userText
                            }
                        ],
                        max_tokens: 150,
                        temperature: 0.7
                    })
                });

                if (!response.ok) {
                    throw new Error(`OpenAI API error: ${response.status}`);
                }

                const data = await response.json();
                return data.choices[0].message.content.trim();
            }

            async speakText(text) {
                return new Promise((resolve, reject) => {
                    if (!('speechSynthesis' in window)) {
                        reject(new Error('Text-to-speech not supported'));
                        return;
                    }

                    const utterance = new SpeechSynthesisUtterance(text);
                    utterance.rate = 0.9;
                    utterance.pitch = 1;
                    utterance.volume = 1;

                    utterance.onend = () => resolve();
                    utterance.onerror = (event) => reject(new Error('Speech synthesis error'));

                    speechSynthesis.speak(utterance);
                });
            }

            addToTranscript(speaker, text) {
                const timestamp = new Date().toLocaleTimeString();
                const entry = { speaker, text, timestamp };
                this.conversationLog.push(entry);

                const transcriptArea = document.getElementById('transcriptArea');
                const entryDiv = document.createElement('div');
                entryDiv.className = `transcript-entry ${speaker.toLowerCase()}-message`;
                entryDiv.innerHTML = `
                    <strong>${speaker}</strong> <span style="opacity: 0.6; font-size: 0.9em;">${timestamp}</span><br>
                    ${text}
                `;

                // Clear placeholder text if it exists
                if (transcriptArea.children.length === 1 && transcriptArea.children[0].style.textAlign === 'center') {
                    transcriptArea.innerHTML = '';
                }

                transcriptArea.appendChild(entryDiv);
                transcriptArea.scrollTop = transcriptArea.scrollHeight;
            }

            logToExternalService(userText, aiResponse) {
                const googleSheetUrl = document.getElementById('google-sheet-url').value.trim();
                
                if (googleSheetUrl) {
                    // Note: This would need a backend service or Google Apps Script to work
                    console.log('Logging to external service:', { userText, aiResponse, timestamp: new Date().toISOString() });
                    this.showSuccess('Conversation logged successfully!');
                } else {
                    console.log('No external logging configured');
                }
            }

            updateButton() {
                const btn = document.getElementById('voiceBtn');
                btn.className = 'voice-button';
                
                if (this.isListening) {
                    btn.className += ' listening';
                    btn.innerHTML = 'üî¥';
                } else if (this.isProcessing) {
                    btn.className += ' processing';
                    btn.innerHTML = '‚öôÔ∏è';
                } else {
                    btn.innerHTML = 'üé§';
                }
            }

            updateStatus(message) {
                document.getElementById('status').textContent = message;
            }

            showError(message) {
                const status = document.getElementById('status');
                status.innerHTML = `<div class="error">${message}</div>`;
            }

            showSuccess(message) {
                const status = document.getElementById('status');
                status.innerHTML = `<div class="success">${message}</div>`;
                setTimeout(() => this.updateStatus('Ready to listen! Click the microphone to start.'), 3000);
            }

            resetState() {
                this.isListening = false;
                this.isProcessing = false;
                this.updateButton();
                this.updateStatus('Ready to listen! Click the microphone to start.');
            }
        }

        // Global functions
        function clearTranscript() {
            document.getElementById('transcriptArea').innerHTML = '<div style="text-align: center; opacity: 0.6;">Conversation will appear here...</div>';
            if (window.voiceAI) {
                window.voiceAI.conversationLog = [];
            }
        }

        function downloadTranscript() {
            if (!window.voiceAI || window.voiceAI.conversationLog.length === 0) {
                alert('No conversation to download!');
                return;
            }

            const transcript = window.voiceAI.conversationLog
                .map(entry => `[${entry.timestamp}] ${entry.speaker}: ${entry.text}`)
                .join('\n\n');

            const blob = new Blob([transcript], { type: 'text/plain' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `voice-ai-transcript-${new Date().toISOString().split('T')[0]}.txt`;
            document.body.appendChild(a);
            a.click();
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
        }

        // Initialize the app
        window.addEventListener('DOMContentLoaded', () => {
            window.voiceAI = new VoiceAIDemo();
        });
    </script>
</body>
</html>